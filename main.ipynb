{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from ast import literal_eval\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from src.utils import *\n",
    "from src.data_utils import SQLDBManager, Neo4jGraphManager\n",
    "from sql_pgvector.chain_utils import RAGChainManager\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully setup Azure OpenAI authentication\n"
     ]
    }
   ],
   "source": [
    "# load env vars\n",
    "host, port, db, user, password = load_postgres_env_variables()\n",
    "# dbm = SQLDBManager.from_env()   # instantiate SQLDBManager\n",
    "setup_azure_openai()    # setup azure openai AD token\n",
    "# print(f\"AD token set: {os.environ['AZURE_OPENAI_AD_TOKEN']}\")\n",
    "table = 'pegadata.ppm_work_filtered'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare & init DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to database successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set db variables\n",
    "# src_table = 'pegadata.ppm_work'\n",
    "# req_cols_path = 'data/pega-as-clone/req_fields.txt'\n",
    "# data_table = f'{src_table}_filtered'\n",
    "# primary_key = 'pxinsname'\n",
    "# text_col = 'pydescription'\n",
    "# instantiate sql db manager & connect to sql db\n",
    "# dbm = SQLDBManager.from_env()\n",
    "\n",
    "# filter 'ppm_work' table and create new (if not exists) \n",
    "# sqldb_manager.filter_table(src_table, req_cols_path, primary_key)\n",
    "# clean text in 'pydescription' before creating embs\n",
    "# sqldb_manager.clean_html(data_table, [text_col], primary_key)\n",
    "\n",
    "# copy ppm_work_filtered table to 'public' schema\n",
    "# copy_table_query = f'''CREATE TABLE public.ppm_work_filtered AS\n",
    "# SELECT * FROM pegadata.ppm_work_filtered;'''\n",
    "# dbm.db.run(copy_table_query)\n",
    "\n",
    "# create embeddings\n",
    "# embs_model = AzureOpenAIEmbeddings(azure_deployment=\"text-embedding-ada-002\") # instantiate embeddings model\n",
    "# sqldb_manager.create_embs_col(data_table, text_col, embs_model) # create embeddings col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to database successfully.\n"
     ]
    }
   ],
   "source": [
    "# instantiate llm & db chain\n",
    "# dbm = SQLDBManager.from_env()\n",
    "# llm = AzureChatOpenAI(model='gpt-35-turbo', max_tokens=500, temperature=0.9)\n",
    "# db_chain = SQLDatabaseChain.from_llm(llm, dbm.db, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate embs model, llm, and rag chain manager\n",
    "embs_model = AzureOpenAIEmbeddings(azure_deployment=\"text-embedding-ada-002\") # instantiate embeddings model\n",
    "llm = AzureChatOpenAI(model='gpt-35-turbo', max_tokens=500, temperature=0.9, model_kwargs={\"stop\":[\"\\nSQLResult:\"]})  # instantiate lm\n",
    "rcm = RAGChainManager(dbm.db, llm, embs_model)   # instantiate rag chain manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: list all user stories with deadlines before Feb 1, 2024, along w their assignees and status\n",
      "\n",
      "sql query: SELECT \"pxinsname\", \"pyoriguserid\", \"pystatuswork\" FROM \"ppm_work_filtered\" WHERE \"pxobjclass\" = 'PegaProjMgmt-Work-UserStory' AND \"pysladeadline\" < date '2024-02-01' LIMIT 5\n",
      "\n",
      "sql query result: [('US-4', 'Administrator', 'Pending-Details'), ('US-6', 'Administrator', 'Pending-Details'), ('US-9', 'Administrator', 'Pending-Details'), ('US-1', 'Administrator', 'Pending-Details'), ('US-10', 'Administrator', 'Pending-Details')]\n",
      "\n",
      "response: The SQL query is selecting the values of \"pxinsname\", \"pyoriguserid\", and \"pystatuswork\" from the \"ppm_work_filtered\" table. It is filtering the results based on the condition that \"pxobjclass\" is equal to 'PegaProjMgmt-Work-UserStory' and \"pysladeadline\" is less than the date '2024-02-01'. The query is limited to returning only 5 rows. The SQL response shows the resulting values for the specified columns: [('US-4', 'Administrator', 'Pending-Details'), ('US-6', 'Administrator', 'Pending-Details'), ('US-9', 'Administrator', 'Pending-Details'), ('US-1', 'Administrator', 'Pending-Details'), ('US-10', 'Administrator', 'Pending-Details')].\n"
     ]
    }
   ],
   "source": [
    "# generate sql query\n",
    "# user_query = \"how many user stories are under epic 4?\"\n",
    "# user_query = \"find the most recent user story under epic 4 and summarize it\"\n",
    "# user_query = \"list all the user stories under epic 4, along w/ their priority (if known) and status\"\n",
    "user_query = \"list all user stories with deadlines before Feb 1, 2024, along w their assignees and status\"\n",
    "# sql_query = rcm.gen_query(user_query)\n",
    "# print(f\"query: {user_query}\\ngenerated sql query: {sql_query}\") \n",
    "\n",
    "# generate response using rag\n",
    "sql_query, response = rcm.gen_response(user_query)\n",
    "query_res = dbm.db.run(sql_query)   # run generated sql query on db \n",
    "print(f\"query: {user_query}\\n\\nsql query: {sql_query}\\n\\nsql query result: {query_res}\\n\\nresponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KG-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create neo4j graph from postgres db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngm = Neo4jGraphManager.from_env()  # instantiate neo4j graph manager\n",
    "# ngm.from_table(table, reset=False)    # create graph from table\n",
    "# ngm.graph.refresh_schema()\n",
    "# print(f\"graph schema:\\n{ngm.graph.schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement KG-RAG using graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user query: find the most recent user story under epic 4 and summarize it\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (us:Object {pxobjclass: 'US'})-[:BELONGS_TO_EPIC]->(e:Object {pxinsname: 'EPIC-4'})\n",
      "RETURN us.pxinsname AS UserStory, us.pxcreatedatetime AS CreationDate, us.pydescription AS Description\n",
      "ORDER BY us.pxcreatedatetime DESC\n",
      "LIMIT 1\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'UserStory': 'US-10', 'CreationDate': neo4j.time.DateTime(2023, 12, 11, 18, 0, 58, 414000000), 'Description': 'As SRT I would like to have mapping stored in cpsetting updated and pointing\\nto correct API management in Production Adoption and Production account\\n\\n'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The most recent user story under epic 4 is US-10, created on December 11, 2023. The description of the user story is: \"As SRT I would like to have mapping stored in cpsetting updated and pointing to correct API management in Production Adoption and Production account\".\n"
     ]
    }
   ],
   "source": [
    "# generate response using kg rag\n",
    "llm = AzureChatOpenAI(model='gpt-4', max_tokens=500, temperature=0)    # instantiate llm\n",
    "\n",
    "# user_query = \"how many user stories are under epic 4?\"\n",
    "# user_query = \"find the most recent user story under epic 4 and summarize it\"\n",
    "# user_query = \"list all the user stories under epic 4, along w/ their priority (if known) and status\"\n",
    "# user_query = \"list all user stories with deadlines before Feb 1, 2024, along w their assignees and status\"\n",
    "print(f\"user query: {user_query}\")\n",
    "\n",
    "prompt_template = \"\"\"Task: Generate Cypher statement to query a graph database. \n",
    "Instructions:\\n Use only the provided relationship types and properties in the schema. \\nSchema:\\n{schema}\\n\n",
    "Extra info: The primary key is the \"pxinsname\" property of the node, which is generally of the form \"<objclass>-<id>\", where <objclass> = \"US\" for user stories, \"EPIC\" for epics, and \"GOAL\" for goals. The <objclass> of a node is accessible via the \"pxobjclass\" property. A node may also have properties with keys of the form \"<objclass>id\" where the value is the id (i.e. the pxinsname) of the associated node. For example, a user story node may have a property \"epicid\" with value \"EPIC-1\" to indicate that the user story is under the epic with id \"EPIC-1\".\n",
    "Do not include any text except the generated Cypher statement.\n",
    "The question is: {question}\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"schema\", \"question\"], template=prompt_template)   # construct prompt template\n",
    "\n",
    "kg_rag_chain = GraphCypherQAChain.from_llm(llm, graph=ngm.graph, cypher_prompt=prompt, verbose=True)   # instantiate kg rag chain\n",
    "\n",
    "response = kg_rag_chain.run(user_query)  # generate response\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
